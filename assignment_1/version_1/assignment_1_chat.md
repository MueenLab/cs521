# Assignment 1 Chat

**STUDENT**: I am taking a graduate level data mining class. Now I am working on the first assignment, which is about dimensionality reduction. I need your help. First of all, my data is stored in ./data/Asgmnt1_data.txt, can you implement the load_data function in the jupyter notebook, then read the file?

**AI AGENT**: Certainly! Here is an implementation for the `load_data` function that reads your data from Asgmnt1_data.txt using NumPy. This assumes your data is stored as a plain text file with rows as samples and columns as features (e.g., space or tab separated).

Add this code to your notebook:

````python
def load_data(file_path):
    """
    Load the dataset from a given file path.
    Args:
        file_path: str, path to the data file
    Returns:
        data: numpy array of shape (n_samples, n_features)
    """
    data = np.loadtxt(file_path)
    return data

# Example usage:
data = load_data('./data/Asgmnt1_data.txt')
print('Data shape:', data.shape)
````

This will load your data and print its shape. If your file uses a different delimiter (e.g., comma), add the `delimiter=','` argument to `np.loadtxt`.

---

**STUDENT**: I then need to normalized the dataset. Could you please implement `normalize_data()`?

**AI AGENT**: Certainly! Here is an implementation of `normalize_data()` using z-score normalization (feature-wise, i.e., column-wise):

````python
def normalize_data(X):
    """
    Normalize the data matrix X using z-score normalization (column-wise).
    Args:
        X: numpy array of shape (n_samples, n_features)
    Returns:
        X_normalized: numpy array of shape (n_samples, n_features)
    """
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    # Avoid division by zero
    std[std == 0] = 1
    X_normalized = (X - mean) / std
    return X_normalized

# Example usage:
# data_norm = normalize_data(data)
# print('Normalized data shape:', data_norm.shape)
````

This will standardize each feature to have mean 0 and standard deviation 1.

---

**STUDENT**: Then I need to compute the euclidean distance matrix for each pair of data points. Help me implement the function `euclidean_distance_matrix`

**AI AGENT**: Certainly! Here is the `euclidean_distance_matrix` function using explicit loops, as requested:

````python
def euclidean_distance_matrix(X):
    """
    Compute the pairwise Euclidean distance matrix for X using loops.
    Args:
        X: numpy array of shape (n_samples, n_features)
    Returns:
        dist_matrix: numpy array of shape (n_samples, n_samples)
    """
    n_samples = X.shape[0]
    dist_matrix = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(i + 1, n_samples):
            dist = np.sqrt(np.sum((X[i] - X[j]) ** 2))
            dist_matrix[i, j] = dist
            dist_matrix[j, i] = dist  # symmetric
    return dist_matrix

# Example usage:
# dist_matrix = euclidean_distance_matrix(data_norm[:100])
# print('Distance matrix shape:', dist_matrix.shape)
````

---

**STUDENT**: Now implement the Haar matrix function

**AI AGENT**: 

````python
def haar_matrix(n: int):
    """
    Generate an n x n Haar matrix.
    Args:
        n: int, size of the Haar matrix (must be a power of 2)
    Returns:
        H: numpy array of shape (n, n)
    """
    if n < 1 or (n & (n - 1)) != 0:
        raise ValueError("n must be a power of 2 and >= 1")
    H = np.array([[1.0]])
    while H.shape[0] < n:
        H = np.kron(H, [[1, 1], [1, -1]])
    H = H / np.sqrt(n)
    return H

# Example usage:
# H = haar_matrix(8)
# print(H)
````

**STUDENT**: why your function gives a haar matrix with floating points? In my class, the haar matrix contains 0, 1 and -1

**AI AGENT**: The Haar matrix is often defined with entries of 0, 1, and -1 for simplicity and clarity in teaching, especially for integer arithmetic or binary signals. However, for mathematical correctness and orthonormality, the Haar matrix is usually normalized so that its rows are orthonormal vectors. This normalization introduces scaling factors (such as dividing by âˆšn), resulting in floating-point values.

**Summary:**
- **With normalization (floating points):** Ensures the matrix is orthonormal (rows have unit length and are mutually orthogonal). This is standard in signal processing and most textbooks.
- **Without normalization (integers):** Easier to see the structure, but the matrix is not orthonormal.

If your assignment or class expects only 0, 1, and -1, you can remove the normalization step (`H = H / np.sqrt(n)`). If you want the orthonormal version (which is standard for transforms), keep the normalization.

**Let me know which version you need, and I can provide the code accordingly!**

---

**STUDENT**: Now please implement the wavelent transform function

**AI AGENT**: 

````python
def wavelet_transform(X, haar_n: int, reduced_n: int):
    """
    Apply Haar wavelet transform to X using an n x n Haar matrix, and reduce the number of features to `reduced_n`.

    Args:
        X: numpy array of shape (n_samples, n_features)
        haar_n: int, number of features to be passed to haar_matrix()
        reduced_n: int, number of features to keep after wavelet transform
    Returns:
        X_wavelet: numpy array of shape (n_samples, reduced_n)
    """
    # Generate Haar matrix (with 0, 1, -1 entries)
    H = haar_matrix(haar_n)
    # Apply the transform: project data onto Haar basis
    X_wavelet_full = np.dot(X, H.T)
    # Keep only the first reduced_n features
    X_wavelet = X_wavelet_full[:, :reduced_n]
    return X_wavelet

# Example usage:
# data_wavelet = wavelet_transform(data_norm, data_norm.shape[1], reduced_n=8)
# print('Wavelet-transformed data shape:', data_wavelet.shape)
````

---

**STUDENT**: Please implement the pca function

**AI AGENT**: 

````python
def pca(X, n_components: int):
    """
    Perform PCA on X and return the projected data with the top n_components.
    Args:
        X: numpy array of shape (n_samples, n_features)
        n_components: int, number of principal components to keep
    Returns:
        X_pca: numpy array of shape (n_samples, n_components)
    """
    # Center the data
    X_centered = X - np.mean(X, axis=0)
    # Compute covariance matrix
    cov = np.cov(X_centered, rowvar=False)
    # Eigen decomposition
    eigvals, eigvecs = np.linalg.eigh(cov)
    # Sort eigenvectors by eigenvalues in descending order
    idx = np.argsort(eigvals)[::-1]
    eigvecs = eigvecs[:, idx]
    # Select the top n_components
    components = eigvecs[:, :n_components]
    # Project the data
    X_pca = np.dot(X_centered, components)
    return X_pca

# Example usage:
# data_pca = pca(data_norm, 8)
# print('PCA-transformed data shape:', data_pca.shape)
````

---

**STUDENT**: Now i need to evaluate PCA and wavelet transform. First, can you compare the time used for computing the euclidean distance on the following cases:
1. on the original dataset
2. on the wavelet transformed dataset
3. on the PCA transformed dataset?

**AI AGENT**: 

````python
import time

# Use a subset for demonstration if the dataset is large (e.g., first 100 samples)
subset = 100
X_orig = data_norm[:subset]
X_wavelet = wavelet_transform(X_orig, haar_n=X_orig.shape[1], reduced_n=4)
X_pca = pca(X_orig, n_components=4)

# 1. Time for original dataset
start = time.time()
dist_orig = euclidean_distance_matrix(X_orig)
time_orig = time.time() - start
print(f"Time for Euclidean distance (original): {time_orig:.4f} seconds")

# 2. Time for wavelet-transformed dataset
start = time.time()
dist_wavelet = euclidean_distance_matrix(X_wavelet)
time_wavelet = time.time() - start
print(f"Time for Euclidean distance (wavelet): {time_wavelet:.4f} seconds")

# 3. Time for PCA-transformed dataset
start = time.time()
dist_pca = euclidean_distance_matrix(X_pca)
time_pca = time.time() - start
print(f"Time for Euclidean distance (PCA): {time_pca:.4f} seconds")
````

This code will print the elapsed time for each case. Adjust `subset` as needed for your hardware.

---

**STUDENT**: For each pair of cells in the distance matrix, check if the distance relationships (greater or less) hold in the reduced matrices compared to the original matrix.

*Example*: If the distance between points A and B is greater than the distance between A and C in the original matrix, check if this relationship holds in the reduced matrices as well. We want to see how often these relationships are preserved.

**AI AGENT**: Here is a function to compute the **relationship consistency** between the original and reduced distance matrices, as described in your prompt. This function randomly samples triplets (A, B, C) and checks if the relationship (e.g., dist(A,B) > dist(A,C)) is preserved after dimensionality reduction.

````python
import random

def relationship_consistency(dist_orig, dist_reduced, num_samples=10000, seed=42):
    """
    Compare pairwise distance relationships between original and reduced distance matrices.
    Returns the fraction of relationships preserved.
    Args:
        dist_orig: numpy array, shape (n, n), original distance matrix
        dist_reduced: numpy array, shape (n, n), reduced distance matrix
        num_samples: int, number of random triplets to sample
        seed: int, random seed for reproducibility
    Returns:
        consistency: float, fraction of preserved relationships
    """
    np.random.seed(seed)
    n = dist_orig.shape[0]
    count = 0
    preserved = 0
    for _ in range(num_samples):
        # Randomly pick anchor, B, C (all different)
        a, b, c = np.random.choice(n, 3, replace=False)
        rel_orig = dist_orig[a, b] > dist_orig[a, c]
        rel_reduced = dist_reduced[a, b] > dist_reduced[a, c]
        if rel_orig == rel_reduced:
            preserved += 1
        count += 1
    return preserved / count

# Example usage:
# cons_wavelet = relationship_consistency(dist_orig, dist_wavelet)
# cons_pca = relationship_consistency(dist_orig, dist_pca)
# print(f"Wavelet consistency: {cons_wavelet:.4f}")
# print(f"PCA consistency: {cons_pca:.4f}")
````

This will give you the fraction of distance relationships that are preserved after dimensionality reduction. Adjust `num_samples` for speed/accuracy tradeoff.