{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29019faae4dae706",
   "metadata": {},
   "source": [
    "# Assignment 1: Dimensionality Reduction\n",
    "\n",
    "The implementations in this notebook were generated using AI. Some functions may be incorrectly implemented, some may not be efficient.\n",
    "\n",
    "Your goals are:\n",
    "- Fix all bugs\n",
    "- Improve efficiency\n",
    "- Complete the missing analysis\n",
    "\n",
    "**IMPORTANT**: Do not change the function names or the function parameters, as they will be used by the grading script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be13b28ca372a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:29:03.519279Z",
     "start_time": "2025-08-05T03:29:03.249276Z"
    }
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af315cc23129f0f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee99ece9aa05a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:27:50.452525Z",
     "start_time": "2025-08-05T03:27:50.450024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (16000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "# You can use numpy to load the data from a file.\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a given file path.\n",
    "    Args:\n",
    "        file_path: str, path to the data file\n",
    "    Returns:\n",
    "        data: numpy array of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "data = load_data('./data/Asgmnt1_data.txt')\n",
    "print('Data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87329a68f18287ce",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "Perform row-wise normalization on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0c796983b229f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data shape: (16000, 128)\n"
     ]
    }
   ],
   "source": [
    "## Normalization\n",
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data matrix X using z-score normalization.\n",
    "    Args:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "    Returns:\n",
    "        X_normalized: numpy array of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    # Avoid division by zero\n",
    "    std[std == 0] = 1\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized\n",
    "\n",
    "# Example usage:\n",
    "data_norm = normalize_data(data)\n",
    "print('Normalized data shape:', data_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de504cd51c8c66e3",
   "metadata": {},
   "source": [
    "## Euclidean Distance Matrix\n",
    "\n",
    "Implement the function below to compute the Euclidean distance matrix for a given data matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec16d23b0f842280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix shape: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance_matrix(X):\n",
    "    \"\"\"\n",
    "    Compute the pairwise Euclidean distance matrix for X.\n",
    "    Args:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "    Returns:\n",
    "        dist_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    dist_matrix = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            dist = np.sqrt(np.sum((X[i] - X[j]) ** 2))\n",
    "            dist_matrix[i, j] = dist\n",
    "            dist_matrix[j, i] = dist  # symmetric\n",
    "    return dist_matrix\n",
    "\n",
    "# Example usage (you can uncomment this after implementing for testing):\n",
    "dist_matrix = euclidean_distance_matrix(data_norm[:100])\n",
    "print('Distance matrix shape:', dist_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52d83203d8fe9a",
   "metadata": {},
   "source": [
    "## Wavelet Transform (Haar)\n",
    "\n",
    "Implement a function to generate a Haar matrix and use it to perform wavelet transform.\n",
    "\n",
    "You need to use the first 4 coefficients to perform the Wavelet transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e2c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1.]\n",
      " [ 1.  1. -1. -1.  1.  1. -1. -1.]\n",
      " [ 1. -1. -1.  1.  1. -1. -1.  1.]\n",
      " [ 1.  1.  1.  1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1. -1.  1. -1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1. -1. -1.  1. -1.  1.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "def haar_matrix(n: int):\n",
    "    \"\"\"\n",
    "    Generate an n x n Haar matrix.\n",
    "    Args:\n",
    "        n: int, size of the Haar matrix (must be a power of 2)\n",
    "    Returns:\n",
    "        H: numpy array of shape (n, n)\n",
    "    \"\"\"\n",
    "    if n < 1 or (n & (n - 1)) != 0:\n",
    "        raise ValueError(\"n must be a power of 2 and >= 1\")\n",
    "    H = np.array([[1.0]])\n",
    "    while H.shape[0] < n:\n",
    "        H = np.kron(H, [[1, 1], [1, -1]])\n",
    "    # H = H / np.sqrt(n) # Normalize the Haar matrix\n",
    "    return H\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "H = haar_matrix(8)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c08c814325ab4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wavelet-transformed data shape: (16000, 4)\n"
     ]
    }
   ],
   "source": [
    "def wavelet_transform(X, haar_n: int, reduced_n: int):\n",
    "    \"\"\"\n",
    "    Apply Haar wavelet transform to X using an n x n Haar matrix, and reduce the number of features to `reduced_n`.\n",
    "\n",
    "    This function should call haar_matrix to generate the Haar matrix.\n",
    "\n",
    "    Args:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        haar_n: int, number of features to be passed to haar_matrix()\n",
    "        reduced_n: int, number of features to keep after wavelet transform\n",
    "    Returns:\n",
    "        X_wavelet: numpy array of shape (n_samples, reduced_n)\n",
    "    \"\"\"\n",
    "    # Generate Haar matrix (with 0, 1, -1 entries)\n",
    "    H = haar_matrix(haar_n)\n",
    "    # Apply the transform: project data onto Haar basis\n",
    "    X_wavelet_full = np.dot(X, H.T)\n",
    "    # Keep only the first reduced_n features\n",
    "    X_wavelet = X_wavelet_full[:, :reduced_n]\n",
    "    return X_wavelet\n",
    "\n",
    "# Example usage:\n",
    "data_wavelet = wavelet_transform(data_norm, data_norm.shape[1], reduced_n=4)\n",
    "print('Wavelet-transformed data shape:', data_wavelet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e785a5cb8c514c",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Implement PCA without using any external libraries like scikit-learn. Please keep the BEST 4  principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9827054935baed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-transformed data shape: (16000, 4)\n"
     ]
    }
   ],
   "source": [
    "def pca(X, n_components: int):\n",
    "    \"\"\"\n",
    "    Perform PCA on X and return the projected data with the top n_components.\n",
    "    Args:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        n_components: int, number of principal components to keep\n",
    "    Returns:\n",
    "        X_pca: numpy array of shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    # Compute covariance matrix\n",
    "    cov = np.cov(X_centered, rowvar=False)\n",
    "    # Eigen decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    # Sort eigenvectors by eigenvalues in descending order\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    # Select the top n_components\n",
    "    components = eigvecs[:, :n_components]\n",
    "    # Project the data\n",
    "    X_pca = np.dot(X_centered, components)\n",
    "    return X_pca\n",
    "\n",
    "# Example usage:\n",
    "data_pca = pca(data_norm, 4)\n",
    "print('PCA-transformed data shape:', data_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106955beb45a08",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "\n",
    "Compare the dimensionality reduction techniques.\n",
    "\n",
    "Please ensure that your analysis is well-structured and well-documented. You can use use plots to visualize the comparison results.\n",
    "For visualizing high dimensional data, you can use t-SNE (from `scikit-learn`, [link here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) to reduce the dimensions to 2D or 3D for plotting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761a425713ecc72",
   "metadata": {},
   "source": [
    "### 1. Elasped Time\n",
    "\n",
    "Compare the elapsed time for each dimensionality reduction technique. You can use the `time` module to measure the execution time of each function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "494e845ffb27191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T16:54:42.344993Z",
     "start_time": "2025-08-06T16:54:42.342312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Euclidean distance (original): 1.0948 seconds\n",
      "Time for Euclidean distance (wavelet): 1.0571 seconds\n",
      "Time for Euclidean distance (PCA): 1.0334 seconds\n"
     ]
    }
   ],
   "source": [
    "# Your implementation for measuring elapsed time goes here\n",
    "import time\n",
    "\n",
    "# Use a subset for demonstration if the dataset is large (e.g., first 100 samples)\n",
    "subset = 1000\n",
    "X_orig = data_norm[:subset]\n",
    "X_wavelet = wavelet_transform(X_orig, haar_n=X_orig.shape[1], reduced_n=4)\n",
    "X_pca = pca(X_orig, n_components=4)\n",
    "\n",
    "# 1. Time for original dataset\n",
    "start = time.time()\n",
    "dist_orig = euclidean_distance_matrix(X_orig)\n",
    "time_orig = time.time() - start\n",
    "print(f\"Time for Euclidean distance (original): {time_orig:.4f} seconds\")\n",
    "\n",
    "# 2. Time for wavelet-transformed dataset\n",
    "start = time.time()\n",
    "dist_wavelet = euclidean_distance_matrix(X_wavelet)\n",
    "time_wavelet = time.time() - start\n",
    "print(f\"Time for Euclidean distance (wavelet): {time_wavelet:.4f} seconds\")\n",
    "\n",
    "# 3. Time for PCA-transformed dataset\n",
    "start = time.time()\n",
    "dist_pca = euclidean_distance_matrix(X_pca)\n",
    "time_pca = time.time() - start\n",
    "print(f\"Time for Euclidean distance (PCA): {time_pca:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fda47efa2c29d5",
   "metadata": {},
   "source": [
    "### 2. Distance Relation Consistency\n",
    "Compare the distance relation consistency of the original data and the reduced data. You can use the Euclidean distance matrix computed earlier to analyze this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a39afa1dd98ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T16:55:43.461789Z",
     "start_time": "2025-08-06T16:55:43.459454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wavelet consistency: 0.4922\n",
      "PCA consistency: 0.9961\n"
     ]
    }
   ],
   "source": [
    "# Your implementation for distance relation consistency goes here\n",
    "import random\n",
    "\n",
    "def relationship_consistency(dist_orig, dist_reduced, num_samples=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Compare pairwise distance relationships between original and reduced distance matrices.\n",
    "    Returns the fraction of relationships preserved.\n",
    "    Args:\n",
    "        dist_orig: numpy array, shape (n, n), original distance matrix\n",
    "        dist_reduced: numpy array, shape (n, n), reduced distance matrix\n",
    "        num_samples: int, number of random triplets to sample\n",
    "        seed: int, random seed for reproducibility\n",
    "    Returns:\n",
    "        consistency: float, fraction of preserved relationships\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = dist_orig.shape[0]\n",
    "    count = 0\n",
    "    preserved = 0\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly pick anchor, B, C (all different)\n",
    "        a, b, c = np.random.choice(n, 3, replace=False)\n",
    "        rel_orig = dist_orig[a, b] > dist_orig[a, c]\n",
    "        rel_reduced = dist_reduced[a, b] > dist_reduced[a, c]\n",
    "        if rel_orig == rel_reduced:\n",
    "            preserved += 1\n",
    "        count += 1\n",
    "    return preserved / count\n",
    "\n",
    "# Example usage:\n",
    "cons_wavelet = relationship_consistency(dist_orig, dist_wavelet)\n",
    "cons_pca = relationship_consistency(dist_orig, dist_pca)\n",
    "print(f\"Wavelet consistency: {cons_wavelet:.4f}\")\n",
    "print(f\"PCA consistency: {cons_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276fd436c77a88a",
   "metadata": {},
   "source": [
    "### 3. Your Own Metric\n",
    "With the help of AI tools, find another metric that can be used to compare the dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819adaa2c44b0950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T16:56:30.323121Z",
     "start_time": "2025-08-06T16:56:30.320557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code, plots, and analysis for your own metric goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
